{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Gabriele Greco\"\n",
    "import random\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate\n",
    "from operator import xor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nim Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply: # take a random row and select random elements (always < k)\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    if(state.rows[row] > state.k):\n",
    "        num_objects = random.randint(1, state.k)\n",
    "    else:\n",
    "        num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)\n",
    "\n",
    "def shortest_row(state: Nim) -> Nimply: # take the shortest row and select random elements if the matches > k otherwise close the row\n",
    "    row = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    if(state.rows[row] > state.k):\n",
    "       num_objects = random.randint(1, state.k)\n",
    "    else:\n",
    "       num_objects = state.rows[row]\n",
    "    return Nimply(row, num_objects)\n",
    "\n",
    "# algorithm taken from professor's code\n",
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n",
    "\n",
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked\n",
    "\n",
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ACTIONS = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}\n",
    "\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, states, alpha=0.15, random_factor=0.2):  # 80% explore, 20% exploit\n",
    "        self.state_history = [((0, 0), 0)]  # state, reward\n",
    "        self.alpha = alpha\n",
    "        self.random_factor = random_factor\n",
    "        self.G = {}\n",
    "        self.init_reward(states)\n",
    "\n",
    "    def init_reward(self, states):\n",
    "        for i, row in enumerate(states):\n",
    "            for j, col in enumerate(row):\n",
    "                self.G[(j, i)] = np.random.uniform(low=1.0, high=0.1)\n",
    "\n",
    "    def choose_action(self, state, allowedMoves):\n",
    "        maxG = -10e15\n",
    "        next_move = None\n",
    "        randomN = np.random.random()\n",
    "        if randomN < self.random_factor:\n",
    "            # if random number below random factor, choose random action\n",
    "            next_move = np.random.choice(allowedMoves)\n",
    "        else:\n",
    "            # if exploiting, gather all possible actions and choose one with the highest G (reward)\n",
    "            for action in allowedMoves:\n",
    "                new_state = tuple([sum(x) for x in zip(state, ACTIONS[action])])\n",
    "                if self.G[new_state] >= maxG:\n",
    "                    next_move = action\n",
    "                    maxG = self.G[new_state]\n",
    "\n",
    "        return next_move\n",
    "\n",
    "    def update_state_history(self, state, reward):\n",
    "        self.state_history.append((state, reward))\n",
    "\n",
    "    def learn(self):\n",
    "        target = 0\n",
    "\n",
    "        for prev, reward in reversed(self.state_history):\n",
    "            self.G[prev] = self.G[prev] + self.alpha * (target - self.G[prev])\n",
    "            target += reward\n",
    "\n",
    "        self.state_history = []\n",
    "\n",
    "        self.random_factor -= 10e-5  # decrease random factor each episode of play\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maze???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ACTIONS = {'U': (-1, 0), 'D': (1, 0), 'L': (0, -1), 'R': (0, 1)}\n",
    "\n",
    "class Maze(object):\n",
    "    def __init__(self):\n",
    "        self.end = (5, 5)\n",
    "        self.maze = np.zeros((6, 6))\n",
    "        self.maze[0, 0] = 2\n",
    "        self.maze[5, :5] = 1\n",
    "        self.maze[:4, 5] = 1\n",
    "        self.maze[2, 2:] = 1\n",
    "        self.maze[3, 2] = 1\n",
    "        self.maze[self.end] = -1\n",
    "        self.robot_position = (0, 0)\n",
    "        self.steps = 0\n",
    "        self.construct_allowed_states()\n",
    "\n",
    "    def print_maze(self):\n",
    "        print('---------------------------------')\n",
    "        for row in self.maze:\n",
    "            for col in row:\n",
    "                if col == 0:\n",
    "                    print(' 0 ', end=\"\") # empty space\n",
    "                elif col == 1:\n",
    "                    print(' X ', end=\"\") # walls\n",
    "                elif col == 2:\n",
    "                    print(' R ', end=\"\") # robot position\n",
    "                elif col == -1:\n",
    "                    print(' E ', end=\"\")\n",
    "            print(\"\\n\")\n",
    "        print('---------------------------------')\n",
    "\n",
    "    def is_allowed_move(self, state, action):\n",
    "        # check allowed move from a given state\n",
    "        y, x = state\n",
    "        y += ACTIONS[action][0]\n",
    "        x += ACTIONS[action][1]\n",
    "        if y < 0 or x < 0 or y > 5 or x > 5:\n",
    "            # if robot will move off the board\n",
    "            return False\n",
    "        # if robot moves into empty space or its original start position\n",
    "        return self.maze[y, x] <= 0 or self.maze[y, x] == 2\n",
    "\n",
    "    def construct_allowed_states(self):\n",
    "        # create a dictionary of allowed states from any position\n",
    "        # using the isAllowedMove() function\n",
    "        # this is so that you don't have to call the function every time\n",
    "        allowed_states = {}\n",
    "        for y, row in enumerate(self.maze):\n",
    "            for x, col in enumerate(row):\n",
    "                # iterate through all spaces\n",
    "                if self.maze[(y,x)] != 1:\n",
    "                    # if the space is not a wall, add it to the allowed states dictionary\n",
    "                    allowed_states[(y,x)] = []\n",
    "                    for action in ACTIONS:\n",
    "                        if self.is_allowed_move((y,x), action) & (action != 0):\n",
    "                            allowed_states[(y,x)].append(action)\n",
    "        self.allowed_states = allowed_states\n",
    "\n",
    "    def update_maze(self, action):\n",
    "        y, x = self.robot_position # get current position\n",
    "        self.maze[y, x] = 0 # set the current position to 0\n",
    "        y += ACTIONS[action][0] # get new position\n",
    "        x += ACTIONS[action][1] # get new position\n",
    "        self.robot_position = (y, x) # set new position\n",
    "        self.maze[y, x] = 2 # set new position\n",
    "        self.steps += 1 # add steps\n",
    "\n",
    "    def is_game_over(self):\n",
    "        # check if robot in the final position\n",
    "        return self.robot_position == self.end\n",
    "\n",
    "    def get_state_and_reward(self):\n",
    "        return self.robot_position, self.give_reward()\n",
    "\n",
    "    def give_reward(self):\n",
    "        # if at end give 0 reward\n",
    "        # if not at end give -1 reward\n",
    "        return -1 * int(not self.robot_position == self.end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brutally stolen and corrected from\n",
    "# https://towardsdatascience.com/hands-on-introduction-to-reinforcement-learning-in-python-da07f7aaca88\n",
    "\n",
    "# ... and a little bit modified\n",
    "\n",
    "# Same goes for Maze and RLAgent, obviously\n",
    "\n",
    "from Maze import Maze\n",
    "from RLAgent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    maze = Maze()\n",
    "    robot = Agent(maze.maze, alpha=0.1, random_factor=0.4)\n",
    "    moveHistory = []\n",
    "    indices = []\n",
    "\n",
    "    maze.print_maze()\n",
    "\n",
    "    for i in range(5000):\n",
    "\n",
    "        while not maze.is_game_over():\n",
    "            state, _ = maze.get_state_and_reward()  # get the current state\n",
    "            # choose an action (explore or exploit)\n",
    "            action = robot.choose_action(state, maze.allowed_states[state])\n",
    "            maze.update_maze(action)  # update the maze according to the action\n",
    "            state, reward = maze.get_state_and_reward()  # get the new state and reward\n",
    "            # update the robot memory with state and reward\n",
    "            robot.update_state_history(state, reward)\n",
    "            if maze.steps > 1000:\n",
    "                # end the robot if it takes too long to find the goal\n",
    "                maze.robot_position = (5, 5)\n",
    "        robot.learn()  # robot should learn after every episode\n",
    "        # get a history of number of steps taken to plot later\n",
    "        if i % 50 == 0:\n",
    "            print(f\"{i}: {maze.steps}\")\n",
    "            moveHistory.append(maze.steps)\n",
    "            indices.append(i)\n",
    "        maze = Maze()  # reinitialize the maze\n",
    "\n",
    "plt.semilogy(indices, moveHistory, \"b\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, states, alpha = 0.15, random_factor = 0.2):\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(N, turn):\n",
    "    countwin = 0\n",
    "    while(N):\n",
    "        rows = random.randint(3, 4) # number of rows\n",
    "        k = random.randint(1, 3) # upperbound of selected matches\n",
    "        #rows = random.randint(4, 13) # number of rows\n",
    "        #k = random.randint(3, 8) # upperbound of selected matches\n",
    "        #rows = 4\n",
    "        #k = 1\n",
    "        if(turn == 0): # who starts first\n",
    "           player = 1\n",
    "        else:\n",
    "           player = 0\n",
    "        nim = Nim(rows, k) #creating the nim\n",
    "\n",
    "        while(nim):\n",
    "           if(player == 0):\n",
    "              #ply = pure_random(nim)\n",
    "              ply = optimal_strategy(nim)\n",
    "              #ply = shortest_row(nim)\n",
    "           else:\n",
    "              ply = find_best_move(nim)\n",
    "           nim.nimming(ply)\n",
    "           logging.info(f\"Board after player {player} -> {nim}\")\n",
    "           player = 1 - player\n",
    "        if(1 - player == 1): # winner = 1 - player\n",
    "               countwin += 1  \n",
    "        N -= 1             \n",
    "\n",
    "    return countwin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "for N in [1]: # game to played as First Player\n",
    "    countwin = play(N, 0)\n",
    "    logging.info(f\"Game played = {N}: \" f\"Winrate 1° player = {(countwin/N)*100}% \")\n",
    "for N in [1]: # game to played as Second Player\n",
    "    countwin = play(N, 1)\n",
    "    logging.info(f\"Game played = {N}: \" f\"Winrate 2° player = {(countwin/N)*100}% \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb79ab85006438da29af4cfb9f533733debcbb631a03e8ea803cabccdaccbcf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
