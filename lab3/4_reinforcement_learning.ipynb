{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Gabriele Greco\"\n",
    "import random\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate\n",
    "from itertools import combinations_with_replacement\n",
    "from operator import xor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nim Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "        self._end = [i*0 for i in range(num_rows)] # end of nim\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "    \n",
    "    def possible_moves(self):\n",
    "        return [(r, o) for r, c in enumerate(self._rows) for o in range(1, c + 1) if self._k is None or o <= self._k]\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None: # this is update states\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects\n",
    "\n",
    "    def get_reward(self): # 1 if I have won, -1 lost, 0 not won or lost\n",
    "        if (self._rows == self._end):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1 * int(not self._rows == self._end) # -1 or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_strategy(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    return Nimply(row, 1)\n",
    "\n",
    "def pure_random(state: Nim) -> Nimply: # take a random row and select random elements (always < k)\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    if(state.rows[row] > state.k):\n",
    "        num_objects = random.randint(1, state.k)\n",
    "    else:\n",
    "        num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects)\n",
    "\n",
    "def shortest_row(state: Nim) -> Nimply: # take the shortest row and select random elements if the matches > k otherwise close the row\n",
    "    row = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    if(state.rows[row] > state.k):\n",
    "       num_objects = random.randint(1, state.k)\n",
    "    else:\n",
    "       num_objects = state.rows[row]\n",
    "    return Nimply(row, num_objects)\n",
    "\n",
    "# algorithm taken from professor's code\n",
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n",
    "\n",
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = state.possible_moves()\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked\n",
    "\n",
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, state, alpha = 0.15, random_factor = 0.2): # we can modify these values\n",
    "        self.state_history = [((0, 0), 0)] # row, k taken, reward\n",
    "        self.alpha = alpha\n",
    "        self.random_factor = random_factor\n",
    "        self.G = {} # Initialization is done when he meet a new state\n",
    "        self.init_reward(state)\n",
    "            \n",
    "    # G = a set of positions. Distance from our move to our target\n",
    "    def init_reward(self, state):\n",
    "        self.G[0, 0] = 0\n",
    "        for i in state.possible_moves():\n",
    "               self.G[i[0], i[1]] = np.random.uniform(low = 0.1, high = 1.0)\n",
    "\n",
    "    def choose_action(self, state: Nim, possibleMoves):\n",
    "        maxG = -10e15 # very low value\n",
    "        next_move = None\n",
    "        if random.random() < self.random_factor:\n",
    "            next_move = random.choice(possibleMoves)\n",
    "        else:\n",
    "            for action in possibleMoves:\n",
    "                new_state = tuple([action[0], action[1]]) # nuovo stato\n",
    "                if self.G[new_state] >= maxG:\n",
    "                    next_move = action\n",
    "                    maxG = self.G[new_state]\n",
    "                    \n",
    "        return next_move\n",
    "\n",
    "    def update_state_history(self, state, reward): # state = rows\n",
    "        self.state_history.append((state, reward))\n",
    "\n",
    "    def learn(self):\n",
    "        target = 0\n",
    "\n",
    "        for prev, reward in reversed(self.state_history): # travel the history backwards\n",
    "            self.G[prev] = self.G[prev] + self.alpha * (target - self.G[prev])\n",
    "            target += reward\n",
    "        \n",
    "        self.state_history = []\n",
    "        self.random_factor -= 10e-5 # decrease random factor each episode of play"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Game played = 0: Winrate = 100.0% \n",
      "INFO:root:Game played = 100: Winrate = 60.0% \n",
      "INFO:root:Game played = 200: Winrate = 59.0% \n",
      "INFO:root:Game played = 300: Winrate = 49.0% \n",
      "INFO:root:Game played = 400: Winrate = 49.0% \n",
      "INFO:root:Game played = 500: Winrate = 51.0% \n",
      "INFO:root:Game played = 600: Winrate = 55.00000000000001% \n",
      "INFO:root:Game played = 700: Winrate = 59.0% \n",
      "INFO:root:Game played = 800: Winrate = 56.00000000000001% \n",
      "INFO:root:Game played = 900: Winrate = 63.0% \n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "countwin = 0 \n",
    "countgames = 0\n",
    "rows = 6\n",
    "k = 3\n",
    "nim = Nim(rows, k)\n",
    "robot = Agent(nim, alpha=0.1, random_factor=0.4)\n",
    "\n",
    "for n in range(1000):\n",
    "    player = 0 # first player\n",
    "    nim = Nim(rows, k) # initializing nim\n",
    "\n",
    "    while(nim):\n",
    "\n",
    "        if(player == 0):\n",
    "            #ply = optimal_strategy(nim)\n",
    "            #ply = pure_random(nim)\n",
    "            #ply = shortest_row(nim)\n",
    "            ply = simple_strategy(nim)\n",
    "            nim.nimming(ply)\n",
    "        else:\n",
    "            action = robot.choose_action(nim, nim.possible_moves()) # chose the action\n",
    "            nim.nimming(action)  # update nim according to the action\n",
    "            reward = nim.get_reward()  # get the new reward\n",
    "            robot.update_state_history(action, reward) # update the robot memory with state and reward\n",
    "        player = 1 - player\n",
    "\n",
    "\n",
    "    if(1 - player == 1): # winner = 1 - player\n",
    "        countwin += 1\n",
    "    else:\n",
    "         robot.update_state_history(action, -1) # update the robot memory with state and reward\n",
    "\n",
    "    # end of game, let's learn\n",
    "    robot.learn()\n",
    "    countgames += 1\n",
    "    \n",
    "    if n  % 100 == 0:\n",
    "        logging.info(f\"Game played = {n}: \" f\"Winrate = {(countwin/(countgames))*100}% \")\n",
    "        countwin = 0\n",
    "        countgames = 0\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('computational-intelligence-2022-2023-s3034-iK13_okX-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "808a7143e02710871dc7347235595351da5e418e9ed64792647b286ac74d77cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
